{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.utils import np_utils\n",
    "from resize_image import resize_image\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_IMAGES_FOLDER = \"przetworzone_obrazy\"\n",
    "MODEL_FILENAME = \"model_nn.hdf5\"\n",
    "MODEL_LABELS_FILENAME = \"model_labels.dat\"\n",
    "\n",
    "IMAGE_SIZE_X = 20\n",
    "IMAGE_SIZE_Y = 20\n",
    "\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER):\n",
    "  \n",
    "    # Odczyt pliku za pomoca opencv \n",
    "    image = cv2.imread(image_file)\n",
    "    # Sprawdzenie czy udalo sie zaladowac plik\n",
    "    try:\n",
    "        if image.size == 0 :\n",
    "            continue\n",
    "    except:\n",
    "        print(\"Not loaded\")\n",
    "        continue\n",
    "    # Przetworzenie obrazu z RBG do odcieni szarosci\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "    # Skalowanie zdjecia do wielkosci 20x20\n",
    "    image = resize_image(image, IMAGE_SIZE_X, IMAGE_SIZE_Y)\n",
    "\n",
    "    # Przetworzenie zdjecia tak aby ostatecznie znajdowalo sie w 3 wymiarach (20,20,1)\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    # Pobranie nazwy pliku ze sciezki \n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Dodanie obrazu do listy \n",
    "    data.append(image)\n",
    "\n",
    "    # Dodanie etykiety do listy\n",
    "    labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizacja pikseli\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "\n",
    "# Przekstałcenie etykiet do tablicy z numpy\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Podział danych na uczace i treningowe\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18784 samples, validate on 9252 samples\n",
      "Epoch 1/5\n",
      "18784/18784 [==============================] - 9s 475us/sample - loss: 0.1974 - accuracy: 0.9565 - val_loss: 0.0915 - val_accuracy: 0.9749\n",
      "Epoch 2/5\n",
      "18784/18784 [==============================] - 11s 561us/sample - loss: 0.0132 - accuracy: 0.9980 - val_loss: 0.0192 - val_accuracy: 0.9974\n",
      "Epoch 3/5\n",
      "18784/18784 [==============================] - 8s 412us/sample - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0124 - val_accuracy: 0.9986\n",
      "Epoch 4/5\n",
      "18784/18784 [==============================] - 8s 405us/sample - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0120 - val_accuracy: 0.9990\n",
      "Epoch 5/5\n",
      "18784/18784 [==============================] - 8s 406us/sample - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0131 - val_accuracy: 0.9988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b7d38c7ec8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Przeksztalcenie etykiet do listy i zapis modelu etykiet do pliku\n",
    "lb = LabelBinarizer().fit(Y_train)\n",
    "Y_train = lb.transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "\n",
    "with open(MODEL_LABELS_FILENAME, \"wb\") as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# Tworzenie modelu sieci neuronowej\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=(IMAGE_SIZE_X, IMAGE_SIZE_Y, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Trenowanie seci\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis modelu sieci do pliku\n",
    "model.save(MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
